# -*- coding: utf-8 -*-
"""NLP_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D1QXc90Ef5O8WugIrwu_z88lULiYO39e

# Question 1
"""

import csv
from googleapiclient.discovery import build

# Set up the YouTube Data API client
api_key = 'YOUR_API_KEY'
youtube = build('youtube', 'v3', developerKey=api_key)

# Specify the video ID or URL of the YouTube video you want to extract comments from
video_id = 'VIDEO_ID'

# Retrieve comments from the YouTube video
comments = []
next_page_token = None
while True:
    response = youtube.commentThreads().list(
        part='snippet',
        videoId=video_id,
        pageToken=next_page_token,
        maxResults=100
    ).execute()

    for item in response['items']:
        comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
        comments.append(comment)

    next_page_token = response.get('nextPageToken')
    if not next_page_token:
        break

# Save the comments to a CSV file
with open('youtube_comments.csv', 'w', newline='', encoding='utf-8') as csvfile:
    writer = csv.writer(csvfile)
    writer.writerow(['Comment'])
    writer.writerows([[comment] for comment in comments])

# Analyze the comments to determine the most demanding topic
# You can use any natural language processing or text analysis techniques here
# Some possible approaches include sentiment analysis, topic modeling, or keyword extraction
# Choose the one that suits your requirements and use the comments stored in the 'comments' list

# Example: Count the frequency of words in the comments
word_freq = {}
for comment in comments:
    words = comment.split()
    for word in words:
        word_freq[word] = word_freq.get(word, 0) + 1

# Find the most frequent word
most_demanding_topic = max(word_freq, key=word_freq.get)
print('Most demanding topic:', most_demanding_topic)