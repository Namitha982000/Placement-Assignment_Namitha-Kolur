# -*- coding: utf-8 -*-
"""Untitled13.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19QcZa_8CwY3168zfQzXBScaRda3BuPIT

# Question 9
"""

import torch
import torchvision.transforms as transforms
from PIL import Image

# Load the quantized model
model = torch.jit.load("path/to/quantized/model.pt")

# Define the image transformation
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

# Preprocess the input image
def preprocess_image(image_path):
    image = Image.open(image_path)
    image = transform(image)
    image = image.unsqueeze(0)  # Add batch dimension
    return image

# Perform model inference
def inference(image_path):
    image = preprocess_image(image_path)

    # Move the input tensor to the appropriate device (CPU or GPU)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    image = image.to(device)

    # Run the quantized model
    with torch.no_grad():
        output = model(image)

    # Process the output and obtain the predictions

    return predictions

# Example usage
image_path = "path/to/test/image.jpg"
predictions = inference(image_path)
print(predictions)